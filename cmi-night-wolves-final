{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport polars as pl\nimport pandas as pd\nfrom sklearn.base import clone\nfrom scipy.optimize import minimize\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nfrom sklearn.impute import SimpleImputer, KNNImputer\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import Adam\n\nfrom sklearn.preprocessing import StandardScaler\nimport re\nfrom colorama import Fore, Style\n\nfrom tqdm import tqdm\nfrom IPython.display import clear_output\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import VotingRegressor\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score\n\nSEED = 42\nn_splits = 5\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:00:09.946988Z","iopub.execute_input":"2024-12-20T08:00:09.947901Z","iopub.status.idle":"2024-12-20T08:00:09.959002Z","shell.execute_reply.started":"2024-12-20T08:00:09.947858Z","shell.execute_reply":"2024-12-20T08:00:09.957733Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Viewing the data","metadata":{}},{"cell_type":"markdown","source":"### Training Data\nThe training set has 3960 samples with 80 features excluding 'sii' and 'id'. By viewing the training dataset, we can see that there seem to be some NaN features in some rows.","metadata":{}},{"cell_type":"code","source":"season_dtype = pl.Enum(['Spring', 'Summer', 'Fall', 'Winter'])\n\ntrain = (\n    pl.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n    .with_columns(pl.col('^.*Season$').cast(season_dtype))\n)\n\ntest = (\n    pl.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n    .with_columns(pl.col('^.*Season$').cast(season_dtype))\n)\n\ntrain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:00:09.961077Z","iopub.execute_input":"2024-12-20T08:00:09.961450Z","iopub.status.idle":"2024-12-20T08:00:10.033105Z","shell.execute_reply.started":"2024-12-20T08:00:09.961417Z","shell.execute_reply":"2024-12-20T08:00:10.031890Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Missing Values\nAll columns seem to have a large amount of missing values, except for 'id' as well as 'sex', 'age' and season of enrollment. Currently, I suppose we can replace the missing values with it's neightbors using **KNNImputer**. From the definition of scikit-learn: \n_\"Imputation for completing missing values using k-Nearest Neighbors. Each sampleâ€™s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close.\"_","metadata":{}},{"cell_type":"code","source":"missing_count = (\n    train\n    .null_count()\n    .transpose(include_header=True,\n               header_name='feature',\n               column_names=['null_count'])\n    .sort('null_count', descending=True)\n    .with_columns((pl.col('null_count') / len(train)).alias('null_ratio'))\n)\nplt.figure(figsize=(6, 15))\nplt.title('Missing values over the whole training dataset')\nplt.barh(np.arange(len(missing_count)), missing_count.get_column('null_ratio'), color='coral', label='missing')\nplt.barh(np.arange(len(missing_count)), \n         1 - missing_count.get_column('null_ratio'),\n         left=missing_count.get_column('null_ratio'),\n         color='darkseagreen', label='available')\nplt.yticks(np.arange(len(missing_count)), missing_count.get_column('feature'))\nplt.gca().xaxis.set_major_formatter(PercentFormatter(xmax=1, decimals=0))\nplt.xlim(0, 1)\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:00:10.034526Z","iopub.execute_input":"2024-12-20T08:00:10.034979Z","iopub.status.idle":"2024-12-20T08:00:11.182574Z","shell.execute_reply.started":"2024-12-20T08:00:10.034914Z","shell.execute_reply":"2024-12-20T08:00:11.181131Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Test Set\n\nThe test set does not have an PCIAT columns, as PCIAT columns can be used to directly map to 'sii'\n\n- We should try to predict the target without the PCIAT features\n- We can either directly predict **'sii'** using some type of classification **or** use a regression model to predict **PCIAT_Total** which can then be mapped to **'sii'**","metadata":{}},{"cell_type":"code","source":"(train\n .select(pl.col('PCIAT-PCIAT_Total'))\n .group_by(train.get_column('sii'))\n .agg(pl.col('PCIAT-PCIAT_Total').min().alias('PCIAT-PCIAT_Total min'),\n      pl.col('PCIAT-PCIAT_Total').max().alias('PCIAT-PCIAT_Total max'),\n      pl.col('PCIAT-PCIAT_Total').len().alias('count'))\n .sort('sii')\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:00:11.185651Z","iopub.execute_input":"2024-12-20T08:00:11.186127Z","iopub.status.idle":"2024-12-20T08:00:11.199480Z","shell.execute_reply.started":"2024-12-20T08:00:11.186077Z","shell.execute_reply":"2024-12-20T08:00:11.198122Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Actigraphy\n\nLooking at the file of participant id=417c91e, we can see that:\n- She moves alot as enmo > 2 everyday\n- She is in an environment where lux exceeds 2500 every day.","metadata":{}},{"cell_type":"code","source":"# View the actigraphy\ndef analyze_actigraphy(id, only_one_week=False, small=False):\n    actigraphy = pl.read_parquet(f'/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet/id={id}/part-0.parquet')\n    day = actigraphy.get_column('relative_date_PCIAT') + actigraphy.get_column('time_of_day') / 86400e9\n    sample = train.filter(pl.col('id') == id)\n    age = sample.get_column('Basic_Demos-Age').item()\n    sex = ['boy', 'girl'][sample.get_column('Basic_Demos-Sex').item()]\n    actigraphy = (\n        actigraphy\n        .with_columns(\n            (day.diff() * 86400).alias('diff_seconds'),\n            (np.sqrt(np.square(pl.col('X')) + np.square(pl.col('Y')) + np.square(pl.col('Z'))).alias('norm'))\n        )\n    )\n\n    if only_one_week:\n        start = np.ceil(day.min())\n        mask = (start <= day.to_numpy()) & (day.to_numpy() <= start + 7*3)\n        mask &= ~ actigraphy.get_column('non-wear_flag').cast(bool).to_numpy()\n    else:\n        mask = np.full(len(day), True)\n        \n    if small:\n        timelines = [\n            ('enmo', 'forestgreen'),\n            ('light', 'orange'),\n        ]\n    else:\n        timelines = [\n            ('X', 'm'),\n            ('Y', 'm'),\n            ('Z', 'm'),\n#             ('norm', 'c'),\n            ('enmo', 'forestgreen'),\n            ('anglez', 'lightblue'),\n            ('light', 'orange'),\n            ('non-wear_flag', 'chocolate')\n    #         ('diff_seconds', 'k'),\n        ]\n        \n    _, axs = plt.subplots(len(timelines), 1, sharex=True, figsize=(12, len(timelines) * 1.1 + 0.5))\n    for ax, (feature, color) in zip(axs, timelines):\n        ax.set_facecolor('#eeeeee')\n        ax.scatter(day.to_numpy()[mask],\n                   actigraphy.get_column(feature).to_numpy()[mask],\n                   color=color, label=feature, s=1)\n        ax.legend(loc='upper left', facecolor='#eeeeee')\n        if feature == 'diff_seconds':\n            ax.set_ylim(-0.5, 20.5)\n    axs[-1].set_xlabel('day')\n    axs[-1].xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.tight_layout()\n    axs[0].set_title(f'id={id}, {sex}, age={age}')\n    plt.show()\n\nanalyze_actigraphy('0417c91e', only_one_week=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:00:11.201508Z","iopub.execute_input":"2024-12-20T08:00:11.201960Z","iopub.status.idle":"2024-12-20T08:00:13.868014Z","shell.execute_reply.started":"2024-12-20T08:00:11.201915Z","shell.execute_reply":"2024-12-20T08:00:13.866753Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A boy who only sees daylight once a month and does not move much","metadata":{}},{"cell_type":"code","source":"analyze_actigraphy('5f9dddb4', small=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:00:13.869714Z","iopub.execute_input":"2024-12-20T08:00:13.870105Z","iopub.status.idle":"2024-12-20T08:00:14.430176Z","shell.execute_reply.started":"2024-12-20T08:00:13.870069Z","shell.execute_reply":"2024-12-20T08:00:14.428808Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Strange ramps in between the lights $\\rightarrow$ Maybe some missing data was filled in.","metadata":{}},{"cell_type":"code","source":"analyze_actigraphy('bc4eaf77', small=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:00:14.431984Z","iopub.execute_input":"2024-12-20T08:00:14.432495Z","iopub.status.idle":"2024-12-20T08:00:15.421989Z","shell.execute_reply.started":"2024-12-20T08:00:14.432447Z","shell.execute_reply":"2024-12-20T08:00:15.420975Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can conclude that the actigraphy need some data cleaning","metadata":{}},{"cell_type":"code","source":"def process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    df.drop('step', axis=1, inplace=True)\n    return df.describe().values.reshape(-1), filename.split('=')[1]\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    \n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    \n    stats, indexes = zip(*results)\n    \n    df = pd.DataFrame(stats, columns=[f\"Stat_{i}\" for i in range(len(stats[0]))])\n    df['id'] = indexes\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:00:15.423875Z","iopub.execute_input":"2024-12-20T08:00:15.424388Z","iopub.status.idle":"2024-12-20T08:00:15.435049Z","shell.execute_reply.started":"2024-12-20T08:00:15.424339Z","shell.execute_reply":"2024-12-20T08:00:15.433990Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_autoencoder_DeepDance(input_dim, encoding_dim):\n    input_layer = Input(shape=(input_dim,))\n    x = Dense(encoding_dim*3, activation='relu')(input_layer)\n    x = Dense(encoding_dim*2, activation='relu')(x)\n    encoded = Dense(encoding_dim, activation='relu')(x)\n    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n    autoencoder = Model(inputs=input_layer, outputs=decoded)\n    encoder     = Model(inputs=input_layer, outputs=encoded)\n    autoencoder.compile(optimizer=Adam(), loss='mse')   \n    return autoencoder, encoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:00:15.436769Z","iopub.execute_input":"2024-12-20T08:00:15.437301Z","iopub.status.idle":"2024-12-20T08:00:15.452500Z","shell.execute_reply.started":"2024-12-20T08:00:15.437251Z","shell.execute_reply":"2024-12-20T08:00:15.451281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n    scaler = StandardScaler()\n    df_scaled = scaler.fit_transform(df)\n    input_dim = df_scaled.shape[1]\n    autoencoder, encoder = build_autoencoder_DeepDance(input_dim, encoding_dim)\n    autoencoder.fit(df_scaled, df_scaled, epochs=epochs, batch_size=batch_size, shuffle=True, verbose=1)\n    encoded_data = encoder.predict(df_scaled)\n    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i+1}' for i in range(encoded_data.shape[1])])\n    return df_encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:00:15.456387Z","iopub.execute_input":"2024-12-20T08:00:15.456933Z","iopub.status.idle":"2024-12-20T08:00:15.465338Z","shell.execute_reply.started":"2024-12-20T08:00:15.456867Z","shell.execute_reply":"2024-12-20T08:00:15.464158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_origin = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest_origin = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample_origin = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\ntrain_ts_origin = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts_origin = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:00:15.467331Z","iopub.execute_input":"2024-12-20T08:00:15.467752Z","iopub.status.idle":"2024-12-20T08:01:43.194056Z","shell.execute_reply.started":"2024-12-20T08:00:15.467691Z","shell.execute_reply":"2024-12-20T08:01:43.192737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_origin.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:03:49.704270Z","iopub.execute_input":"2024-12-20T08:03:49.705355Z","iopub.status.idle":"2024-12-20T08:03:49.711936Z","shell.execute_reply.started":"2024-12-20T08:03:49.705308Z","shell.execute_reply":"2024-12-20T08:03:49.710814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_origin.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:03:58.574242Z","iopub.execute_input":"2024-12-20T08:03:58.575042Z","iopub.status.idle":"2024-12-20T08:03:58.581492Z","shell.execute_reply.started":"2024-12-20T08:03:58.574996Z","shell.execute_reply":"2024-12-20T08:03:58.580240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ts_origin.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:04:18.171661Z","iopub.execute_input":"2024-12-20T08:04:18.172117Z","iopub.status.idle":"2024-12-20T08:04:18.179528Z","shell.execute_reply.started":"2024-12-20T08:04:18.172079Z","shell.execute_reply":"2024-12-20T08:04:18.178405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_ts_origin.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:04:28.809619Z","iopub.execute_input":"2024-12-20T08:04:28.810041Z","iopub.status.idle":"2024-12-20T08:04:28.816995Z","shell.execute_reply.started":"2024-12-20T08:04:28.809997Z","shell.execute_reply":"2024-12-20T08:04:28.815866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = train_origin.copy()\ntest = test_origin.copy()\nsample = sample_origin.copy()\n\ntrain_ts = train_ts_origin.copy()\ntest_ts = test_ts_origin.copy()\n\ntime_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")\n\ndf_train = train_ts.drop('id', axis=1)\ndf_test = test_ts.drop('id', axis=1)\n\ntrain_ts_encoded = perform_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\ntest_ts_encoded = perform_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)\n\ntime_series_cols = train_ts_encoded.columns.tolist()\ntrain_ts_encoded[\"id\"]=train_ts[\"id\"]\ntest_ts_encoded['id']=test_ts[\"id\"]\n\ntrain = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\ntest = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\n\nfeaturesCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n\nfeaturesCols += time_series_cols\n\ntrain = train[featuresCols]\ntrain = train.dropna(subset='sii')\n\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', 'Fitness_Endurance-Season', \n          'FGC-Season', 'BIA-Season', 'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n\ndef update(df):\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n        \ntrain = update(train)\ntest = update(test)\n\ndef create_mapping(column, dataset):\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}\n\nfor col in cat_c:\n    mapping_train = create_mapping(col, train)\n    mapping_test = create_mapping(col, test)\n    \n    train[col] = train[col].replace(mapping_train).astype(int)\n    test[col] = test[col].replace(mapping_test).astype(int)\n\n# drop rows with thresholds dropna(thresh=N):\n# Retains rows with at least N non-null values.\n# If a row has fewer than N non-null values, it will be dropped\n\nprint(f'Train Shape : {train.shape} || Test Shape : {test.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:01:43.195643Z","iopub.execute_input":"2024-12-20T08:01:43.195976Z","iopub.status.idle":"2024-12-20T08:01:57.800193Z","shell.execute_reply.started":"2024-12-20T08:01:43.195936Z","shell.execute_reply":"2024-12-20T08:01:57.798975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def feature_engineering(df):\n    # 1. Interaction Features:\n    df['Age_and_BMI'] = df['Basic_Demos-Age'] * df['Physical-BMI']\n    df['BP_Difference'] = df['Physical-Systolic_BP'] - df['Physical-Diastolic_BP']\n    # 2. Aggregated Features:\n    df['Avg_BMI'] = df.groupby('Physical-Season')['Physical-BMI'].transform('mean')\n    df['BMI_Per_Weight'] = df['Physical-BMI'] / df['Physical-Weight']\n    \n    # 3. Temporal Features:\n    df['Seasonal_BP'] = df.groupby('Physical-Season')['Physical-Diastolic_BP'].transform('mean')\n    # 5. Ratios and Proportions:\n    df['BMI_Height_Ratio'] = df['Physical-BMI'] / df['Physical-Height']\n    df['HeartRate_to_Weight'] = df['Physical-HeartRate'] / df['Physical-Weight']\n\n    # 6. Health Metrics:\n    df['Frame_to_BMI'] = df['BIA-BIA_Frame_num'] / df['Physical-BMI']\n    \n    return df\n    \ntrain = feature_engineering(train)\ntest = feature_engineering(test)\n\ntrain.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:01:57.801683Z","iopub.execute_input":"2024-12-20T08:01:57.802024Z","iopub.status.idle":"2024-12-20T08:01:57.828600Z","shell.execute_reply.started":"2024-12-20T08:01:57.801992Z","shell.execute_reply":"2024-12-20T08:01:57.827569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:01:57.830355Z","iopub.execute_input":"2024-12-20T08:01:57.831139Z","iopub.status.idle":"2024-12-20T08:01:57.838578Z","shell.execute_reply.started":"2024-12-20T08:01:57.831089Z","shell.execute_reply":"2024-12-20T08:01:57.837347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" if np.any(np.isinf(train)):\n   train = train.replace([np.inf, -np.inf], np.nan)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:01:57.840089Z","iopub.execute_input":"2024-12-20T08:01:57.840554Z","iopub.status.idle":"2024-12-20T08:01:57.856755Z","shell.execute_reply.started":"2024-12-20T08:01:57.840506Z","shell.execute_reply":"2024-12-20T08:01:57.855635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\ndef TrainML(model_class, test_data):\n    \n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead') # Nelder-Mead | # Powell\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n\n    return submission,model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:01:57.858143Z","iopub.execute_input":"2024-12-20T08:01:57.858493Z","iopub.status.idle":"2024-12-20T08:01:57.874066Z","shell.execute_reply.started":"2024-12-20T08:01:57.858463Z","shell.execute_reply":"2024-12-20T08:01:57.872828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Testing Parameters for first run\nParams = {'learning_rate': 0.03884249148676395, 'max_depth': 12, 'num_leaves': 413, 'min_data_in_leaf': 14,\n           'feature_fraction': 0.7987976913702801, 'bagging_fraction': 0.7602261703576205, 'bagging_freq': 2, \n           'lambda_l1': 4.735462555910575, 'lambda_l2': 4.735028557007343e-06} # CV : 0.4094 | LB : 0.471\n\nXGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,  # Increased from 0.1\n    'reg_lambda': 5,  # Increased from 1\n    'random_state': SEED\n}\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 6,\n    'iterations': 200,\n    'random_seed': SEED,\n    'cat_features': cat_c,\n    'verbose': 0,\n    'l2_leaf_reg': 10  # Increase this value\n}\n\nLight = lgb.LGBMRegressor(**Params,random_state=SEED, verbose=-1,n_estimators=200)\nXGB_Model = XGBRegressor(**XGB_Params)\nCatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n\nvoting_model = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model)\n])\n\nSubmission,model = TrainML(voting_model, test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:01:57.875426Z","iopub.execute_input":"2024-12-20T08:01:57.875778Z","iopub.status.idle":"2024-12-20T08:02:56.390853Z","shell.execute_reply.started":"2024-12-20T08:01:57.875744Z","shell.execute_reply":"2024-12-20T08:02:56.389652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Submission.to_csv('submission.csv', index=False)\nprint(Submission['sii'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:02:56.392555Z","iopub.execute_input":"2024-12-20T08:02:56.392881Z","iopub.status.idle":"2024-12-20T08:02:56.401351Z","shell.execute_reply.started":"2024-12-20T08:02:56.392850Z","shell.execute_reply":"2024-12-20T08:02:56.400265Z"}},"outputs":[],"execution_count":null}]}