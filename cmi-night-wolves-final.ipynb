{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# I. Problem\n\n- The primary goal of this competition is to utilize our training data to predict the Severity Impairment Index (sii), a standard measure for evaluating Problematic Internet Use (PIU).\n- The competition data is compiled into two sources, parquet files containing the accelerometer (actigraphy) series and csv files containing the remaining tabular data.\n- The target variable is the Severity Impairment Index (sii), which reflects the degree of Internet influence on life, categorized into four levels: NONE, MILD, MODERATE, and SEVERE.\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"# II. Import the necessary libraries and load data from input files:","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np\nimport polars as pl\nimport pandas as pd\nimport plotly.express as px\nfrom sklearn.base import clone\nfrom scipy.optimize import minimize\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nfrom sklearn.impute import SimpleImputer, KNNImputer\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom sklearn.preprocessing import StandardScaler\nimport re\nfrom colorama import Fore, Style\n\nfrom tqdm import tqdm\nfrom IPython.display import clear_output\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import VotingRegressor\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score\n\nSEED = 42\nn_splits = 5\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T12:42:05.585496Z","iopub.execute_input":"2024-12-21T12:42:05.585903Z","iopub.status.idle":"2024-12-21T12:42:12.806875Z","shell.execute_reply.started":"2024-12-21T12:42:05.585866Z","shell.execute_reply":"2024-12-21T12:42:12.805827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_origin = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest_origin = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample_origin = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\ndata_dict = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/data_dictionary.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T12:42:25.911979Z","iopub.execute_input":"2024-12-21T12:42:25.912523Z","iopub.status.idle":"2024-12-21T12:42:26.002992Z","shell.execute_reply.started":"2024-12-21T12:42:25.912463Z","shell.execute_reply":"2024-12-21T12:42:26.001825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# III. Viewing the data","metadata":{}},{"cell_type":"code","source":"# Make copies for use so that don't have to reload the data in the future.\ntrain = train_origin.copy()\ntest = test_origin.copy()\nsample = sample_origin.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T12:48:53.51359Z","iopub.execute_input":"2024-12-21T12:48:53.513962Z","iopub.status.idle":"2024-12-21T12:48:53.521264Z","shell.execute_reply.started":"2024-12-21T12:48:53.513928Z","shell.execute_reply":"2024-12-21T12:48:53.519972Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train data","metadata":{}},{"cell_type":"code","source":"display(train.head())\nprint(f\"The train data has the shape: {train.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:22:20.144026Z","iopub.execute_input":"2024-12-21T11:22:20.144996Z","iopub.status.idle":"2024-12-21T11:22:20.157244Z","shell.execute_reply.started":"2024-12-21T11:22:20.144934Z","shell.execute_reply":"2024-12-21T11:22:20.156037Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training Data\nThe training set has 3960 samples with 80 features excluding 'sii' and 'id'. By viewing the training dataset, we can see that there seem to be some NaN features in some rows.","metadata":{}},{"cell_type":"markdown","source":"## Test data","metadata":{}},{"cell_type":"code","source":"display(test.head())\nprint(f\"The test data has the shape: {test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:22:41.976504Z","iopub.execute_input":"2024-12-21T11:22:41.97689Z","iopub.status.idle":"2024-12-21T11:22:41.989475Z","shell.execute_reply.started":"2024-12-21T11:22:41.976856Z","shell.execute_reply":"2024-12-21T11:22:41.987989Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Target Variables and Internet use","metadata":{}},{"cell_type":"markdown","source":"* There are 22 PCIAT features. These comprise answers to 20 questions (each marked out of 5), the total score and 'season' when the test was carried out.\n* The sii target is derived from the total PCIAT score:\n    - 0-30 gives sii = 0\n    - 31-49 gives sii = 1\n    - 50-79 gives sii = 2\n    - 80-100 gives sii = 3.\n* We drop all the PCIAT features from the dataset except the PCIAT Total feature which can be used as a regression target.\n* The PCIAT Total visualisation box plot shows us that many of the top scores look like outliers - yet this is our most important category.","metadata":{}},{"cell_type":"code","source":"data_dict[data_dict['Field'] == 'PCIAT-PCIAT_Total']['Value Labels'].iloc[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:34:23.164975Z","iopub.execute_input":"2024-12-21T11:34:23.16534Z","iopub.status.idle":"2024-12-21T11:34:23.173267Z","shell.execute_reply.started":"2024-12-21T11:34:23.165308Z","shell.execute_reply":"2024-12-21T11:34:23.172072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.sii.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:35:06.309574Z","iopub.execute_input":"2024-12-21T11:35:06.310156Z","iopub.status.idle":"2024-12-21T11:35:06.320463Z","shell.execute_reply.started":"2024-12-21T11:35:06.310101Z","shell.execute_reply":"2024-12-21T11:35:06.319002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.countplot(train, x = 'sii').set_title('Count of sii')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T11:35:34.158071Z","iopub.execute_input":"2024-12-21T11:35:34.158455Z","iopub.status.idle":"2024-12-21T11:35:34.429568Z","shell.execute_reply.started":"2024-12-21T11:35:34.15842Z","shell.execute_reply":"2024-12-21T11:35:34.428467Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Test Set\n\nThe test set does not have an PCIAT columns, as PCIAT columns can be used to directly map to 'sii'\n\n- We should try to predict the target without the PCIAT features\n- We can either directly predict **'sii'** using some type of classification **or** use a regression model to predict **PCIAT_Total** which can then be mapped to **'sii'**","metadata":{}},{"cell_type":"code","source":"(train\n .select(pl.col('PCIAT-PCIAT_Total'))\n .group_by(train.get_column('sii'))\n .agg(pl.col('PCIAT-PCIAT_Total').min().alias('PCIAT-PCIAT_Total min'),\n      pl.col('PCIAT-PCIAT_Total').max().alias('PCIAT-PCIAT_Total max'),\n      pl.col('PCIAT-PCIAT_Total').len().alias('count'))\n .sort('sii')\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T12:49:34.780585Z","iopub.execute_input":"2024-12-21T12:49:34.78154Z","iopub.status.idle":"2024-12-21T12:49:34.803905Z","shell.execute_reply.started":"2024-12-21T12:49:34.781492Z","shell.execute_reply":"2024-12-21T12:49:34.802797Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Catagory Variables","metadata":{}},{"cell_type":"code","source":"season_dtype = pl.Enum(['Spring', 'Summer', 'Fall', 'Winter'])\n\ntrain = (\n    pl.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n    .with_columns(pl.col('^.*Season$').cast(season_dtype))\n)\n\ntest = (\n    pl.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n    .with_columns(pl.col('^.*Season$').cast(season_dtype))\n)\n\ntrain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T12:49:19.461391Z","iopub.execute_input":"2024-12-21T12:49:19.461781Z","iopub.status.idle":"2024-12-21T12:49:19.615671Z","shell.execute_reply.started":"2024-12-21T12:49:19.461742Z","shell.execute_reply":"2024-12-21T12:49:19.614637Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Missing Values\nAll columns seem to have a large amount of missing values, except for 'id' as well as 'sex', 'age' and season of enrollment.","metadata":{}},{"cell_type":"code","source":"missing_count = (\n    train\n    .null_count()\n    .transpose(include_header=True,\n               header_name='feature',\n               column_names=['null_count'])\n    .sort('null_count', descending=True)\n    .with_columns((pl.col('null_count') / len(train)).alias('null_ratio'))\n)\nplt.figure(figsize=(6, 15))\nplt.title('Missing values over the whole training dataset')\nplt.barh(np.arange(len(missing_count)), missing_count.get_column('null_ratio'), color='coral', label='missing')\nplt.barh(np.arange(len(missing_count)), \n         1 - missing_count.get_column('null_ratio'),\n         left=missing_count.get_column('null_ratio'),\n         color='darkseagreen', label='available')\nplt.yticks(np.arange(len(missing_count)), missing_count.get_column('feature'))\nplt.gca().xaxis.set_major_formatter(PercentFormatter(xmax=1, decimals=0))\nplt.xlim(0, 1)\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T12:49:28.254999Z","iopub.execute_input":"2024-12-21T12:49:28.25554Z","iopub.status.idle":"2024-12-21T12:49:29.43975Z","shell.execute_reply.started":"2024-12-21T12:49:28.255494Z","shell.execute_reply":"2024-12-21T12:49:29.438675Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Actigraphy\n\nLooking at the file of participant id=417c91e, we can see that:\n- She moves alot as enmo > 2 everyday\n- She is in an environment where lux exceeds 2500 every day.","metadata":{}},{"cell_type":"code","source":"# View the actigraphy\ndef analyze_actigraphy(id, only_one_week=False, small=False):\n    actigraphy = pl.read_parquet(f'/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet/id={id}/part-0.parquet')\n    day = actigraphy.get_column('relative_date_PCIAT') + actigraphy.get_column('time_of_day') / 86400e9\n    sample = train.filter(pl.col('id') == id)\n    age = sample.get_column('Basic_Demos-Age').item()\n    sex = ['boy', 'girl'][sample.get_column('Basic_Demos-Sex').item()]\n    actigraphy = (\n        actigraphy\n        .with_columns(\n            (day.diff() * 86400).alias('diff_seconds'),\n            (np.sqrt(np.square(pl.col('X')) + np.square(pl.col('Y')) + np.square(pl.col('Z'))).alias('norm'))\n        )\n    )\n\n    if only_one_week:\n        start = np.ceil(day.min())\n        mask = (start <= day.to_numpy()) & (day.to_numpy() <= start + 7*3)\n        mask &= ~ actigraphy.get_column('non-wear_flag').cast(bool).to_numpy()\n    else:\n        mask = np.full(len(day), True)\n        \n    if small:\n        timelines = [\n            ('enmo', 'forestgreen'),\n            ('light', 'orange'),\n        ]\n    else:\n        timelines = [\n            ('X', 'm'),\n            ('Y', 'm'),\n            ('Z', 'm'),\n#             ('norm', 'c'),\n            ('enmo', 'forestgreen'),\n            ('anglez', 'lightblue'),\n            ('light', 'orange'),\n            ('non-wear_flag', 'chocolate')\n    #         ('diff_seconds', 'k'),\n        ]\n        \n    _, axs = plt.subplots(len(timelines), 1, sharex=True, figsize=(12, len(timelines) * 1.1 + 0.5))\n    for ax, (feature, color) in zip(axs, timelines):\n        ax.set_facecolor('#eeeeee')\n        ax.scatter(day.to_numpy()[mask],\n                   actigraphy.get_column(feature).to_numpy()[mask],\n                   color=color, label=feature, s=1)\n        ax.legend(loc='upper left', facecolor='#eeeeee')\n        if feature == 'diff_seconds':\n            ax.set_ylim(-0.5, 20.5)\n    axs[-1].set_xlabel('day')\n    axs[-1].xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.tight_layout()\n    axs[0].set_title(f'id={id}, {sex}, age={age}')\n    plt.show()\n\nanalyze_actigraphy('0417c91e', only_one_week=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T12:31:29.205307Z","iopub.execute_input":"2024-12-20T12:31:29.205715Z","iopub.status.idle":"2024-12-20T12:31:31.761858Z","shell.execute_reply.started":"2024-12-20T12:31:29.205669Z","shell.execute_reply":"2024-12-20T12:31:31.760723Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A boy who only sees daylight once a month and does not move much","metadata":{}},{"cell_type":"code","source":"analyze_actigraphy('5f9dddb4', small=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T12:31:31.763499Z","iopub.execute_input":"2024-12-20T12:31:31.763827Z","iopub.status.idle":"2024-12-20T12:31:32.281438Z","shell.execute_reply.started":"2024-12-20T12:31:31.763795Z","shell.execute_reply":"2024-12-20T12:31:32.280342Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Strange ramps in between the lights $\\rightarrow$ Maybe some missing data was filled in.","metadata":{}},{"cell_type":"code","source":"analyze_actigraphy('bc4eaf77', small=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T12:31:32.28257Z","iopub.execute_input":"2024-12-20T12:31:32.282873Z","iopub.status.idle":"2024-12-20T12:31:33.223697Z","shell.execute_reply.started":"2024-12-20T12:31:32.282843Z","shell.execute_reply":"2024-12-20T12:31:33.222484Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can conclude that the actigraphy need some data cleaning","metadata":{}},{"cell_type":"markdown","source":"# IV. Data Processing\n\n## 1. Combining Two Datasets","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    df.drop('step', axis=1, inplace=True)\n    return df.describe().values.reshape(-1), filename.split('=')[1]\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    \n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    \n    stats, indexes = zip(*results)\n    \n    df = pd.DataFrame(stats, columns=[f\"Stat_{i}\" for i in range(len(stats[0]))])\n    df['id'] = indexes\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T12:31:33.227314Z","iopub.execute_input":"2024-12-20T12:31:33.227654Z","iopub.status.idle":"2024-12-20T12:31:33.235242Z","shell.execute_reply.started":"2024-12-20T12:31:33.22762Z","shell.execute_reply":"2024-12-20T12:31:33.234046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ts_origin = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts_origin = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T12:31:33.23671Z","iopub.execute_input":"2024-12-20T12:31:33.237071Z","iopub.status.idle":"2024-12-20T12:33:05.137224Z","shell.execute_reply.started":"2024-12-20T12:31:33.237037Z","shell.execute_reply":"2024-12-20T12:33:05.135997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = train_origin.copy()\ntest = test_origin.copy()\nsample = sample_origin.copy()\n\ntrain_ts = train_ts_origin.copy()\ntest_ts = test_ts_origin.copy()\n\ntime_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")\n\ntrain = pd.merge(train, train_ts, how=\"left\", on='id')\ntest = pd.merge(test, test_ts, how=\"left\", on='id')\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\n\nfeaturesCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n\nfeaturesCols += time_series_cols\n\ntrain = train[featuresCols]\ntrain = train.dropna(subset='sii')\n\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', 'Fitness_Endurance-Season', \n          'FGC-Season', 'BIA-Season', 'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n# fills missing categorical value with missing \ndef update(df):\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n        \ntrain = update(train)\ntest = update(test)\n\n# create unique mappings\ndef create_mapping(column, dataset):\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}\n\nfor col in cat_c:\n    mapping_train = create_mapping(col, train)\n    mapping_test = create_mapping(col, test)\n    \n    train[col] = train[col].replace(mapping_train).astype(int)\n    test[col] = test[col].replace(mapping_test).astype(int)\n\n# drop rows with thresholds dropna(thresh=N):\n# Retains rows with at least N non-null values.\n# If a row has fewer than N non-null values, it will be dropped\n\nprint(f'Train Shape : {train.shape} || Test Shape : {test.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T12:33:05.138813Z","iopub.execute_input":"2024-12-20T12:33:05.139162Z","iopub.status.idle":"2024-12-20T12:33:05.241758Z","shell.execute_reply.started":"2024-12-20T12:33:05.139126Z","shell.execute_reply":"2024-12-20T12:33:05.240629Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Feature Engineering\nCreation of data using domain knowledge. This function creates new features in the dataset df based on existing columns to improve model performance. It focuses on interaction terms, ratios, and aggregated statistics.","metadata":{}},{"cell_type":"code","source":"def feature_engineering(df):\n    # 1. Interaction Features:\n    df['Age_and_BMI'] = df['Basic_Demos-Age'] * df['Physical-BMI']\n    df['BP_Difference'] = df['Physical-Systolic_BP'] - df['Physical-Diastolic_BP']\n    # 2. Aggregated Features:\n    df['Avg_BMI'] = df.groupby('Physical-Season')['Physical-BMI'].transform('mean')\n    df['BMI_Per_Weight'] = df['Physical-BMI'] / df['Physical-Weight']\n    \n    # 3. Temporal Features:\n    df['Seasonal_BP'] = df.groupby('Physical-Season')['Physical-Diastolic_BP'].transform('mean')\n    # 5. Ratios and Proportions:\n    df['BMI_Height_Ratio'] = df['Physical-BMI'] / df['Physical-Height']\n    df['HeartRate_to_Weight'] = df['Physical-HeartRate'] / df['Physical-Weight']\n\n    # 6. Health Metrics:\n    df['Frame_to_BMI'] = df['BIA-BIA_Frame_num'] / df['Physical-BMI']\n    \n    return df\n    \ntrain = feature_engineering(train)\ntest = feature_engineering(test)\n\ntrain.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T12:33:05.243292Z","iopub.execute_input":"2024-12-20T12:33:05.244192Z","iopub.status.idle":"2024-12-20T12:33:05.270542Z","shell.execute_reply.started":"2024-12-20T12:33:05.24414Z","shell.execute_reply":"2024-12-20T12:33:05.269473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T12:33:05.272099Z","iopub.execute_input":"2024-12-20T12:33:05.272407Z","iopub.status.idle":"2024-12-20T12:33:05.285163Z","shell.execute_reply.started":"2024-12-20T12:33:05.272374Z","shell.execute_reply":"2024-12-20T12:33:05.284075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" if np.any(np.isinf(train)):\n   train = train.replace([np.inf, -np.inf], np.nan)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T12:33:05.286295Z","iopub.execute_input":"2024-12-20T12:33:05.28662Z","iopub.status.idle":"2024-12-20T12:33:05.3044Z","shell.execute_reply.started":"2024-12-20T12:33:05.286586Z","shell.execute_reply":"2024-12-20T12:33:05.303468Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# V. Training the model\n1. Quadratic Weighted Kappa Calculation (quadratic_weighted_kappa):\n\nThis function calculates a metric called the Quadratic Weighted Kappa (QWK).\nQWK is often used to measure the agreement between two ratings (predictions and true labels) while accounting for the possibility of random agreement.\nIt’s commonly used in regression-like problems where the target variable is ordinal (e.g., 0, 1, 2, 3).\n2. Threshold-Based Prediction Rounding (threshold_Rounder):\n\nConverts continuous predictions (e.g., 0.8, 1.4) into discrete classes (0, 1, 2, 3) based on given thresholds.\nThresholds like [0.5, 1.5, 2.5] determine the cut-off points between classes.\nEvaluation of Predictions (evaluate_predictions):\n\nThis function evaluates how well the predictions perform after applying specific thresholds.\nIt returns the negative QWK score so it can be minimized during optimization (the higher the QWK score, the better).\nTraining and Validation (TrainML):\n\n3. Data Preparation: Splits the input train dataset into features (X) and target (y).\n4. Cross-Validation Setup: Uses StratifiedKFold to ensure balanced splits of the target variable across training and validation folds.\n5. Per-Fold Training:\nTrains a cloned version of the model_class on each fold of training data.\nPredicts on both training and validation sets, saving non-rounded (raw) predictions.\nCalculates QWK scores for training and validation predictions (before and after rounding).\nStores predictions for the test dataset.\n6. Threshold Optimization:\nUses the minimize function to find the optimal thresholds for rounding the predictions, maximizing the QWK score.\n7. Final Metrics and Predictions:\nApplies the optimized thresholds to the out-of-fold predictions to compute the final QWK score.\nRounds the test predictions using the same optimized thresholds and prepares the final submission DataFrame.\nOutput:\n\nReturns the final submission DataFrame containing predicted labels for the test dataset.\n","metadata":{}},{"cell_type":"code","source":"def quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\ndef TrainML(model_class, test_data):\n    \n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead') # Nelder-Mead | # Powell\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n\n    return submission,model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T12:33:05.305876Z","iopub.execute_input":"2024-12-20T12:33:05.306413Z","iopub.status.idle":"2024-12-20T12:33:05.321019Z","shell.execute_reply.started":"2024-12-20T12:33:05.306377Z","shell.execute_reply":"2024-12-20T12:33:05.319232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Voting Regression Model\nThis code sets up three popular machine learning models (LightGBM, XGBoost, and CatBoost) with specific hyperparameters, combines them into a Voting Regressor, and trains the ensemble model using the TrainML function. Here's a detailed explanation:","metadata":{}},{"cell_type":"code","source":"# Testing Parameters for first run\nParams = {'learning_rate': 0.03884249148676395, 'max_depth': 12, 'num_leaves': 413, 'min_data_in_leaf': 14,\n           'feature_fraction': 0.7987976913702801, 'bagging_fraction': 0.7602261703576205, 'bagging_freq': 2, \n           'lambda_l1': 4.735462555910575, 'lambda_l2': 4.735028557007343e-06} # CV : 0.4094 | LB : 0.471\n\nXGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,  # Increased from 0.1\n    'reg_lambda': 5,  # Increased from 1\n    'random_state': SEED\n}\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 6,\n    'iterations': 200,\n    'random_seed': SEED,\n    'cat_features': cat_c,\n    'verbose': 0,\n    'l2_leaf_reg': 10  # Increase this value\n}\n\nLight = LGBMRegressor(**Params,random_state=SEED, verbose=-1,n_estimators=200)\nXGB_Model = XGBRegressor(**XGB_Params)\nCatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n\nvoting_model = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model)\n])\n\nSubmission,model = TrainML(voting_model, test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T12:33:05.322773Z","iopub.execute_input":"2024-12-20T12:33:05.323373Z","iopub.status.idle":"2024-12-20T12:34:09.939072Z","shell.execute_reply.started":"2024-12-20T12:33:05.323317Z","shell.execute_reply":"2024-12-20T12:34:09.938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Submission.to_csv('submission.csv', index=False)\nprint(Submission['sii'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T12:34:09.940524Z","iopub.execute_input":"2024-12-20T12:34:09.940965Z","iopub.status.idle":"2024-12-20T12:34:09.953842Z","shell.execute_reply.started":"2024-12-20T12:34:09.940915Z","shell.execute_reply":"2024-12-20T12:34:09.952658Z"}},"outputs":[],"execution_count":null}]}